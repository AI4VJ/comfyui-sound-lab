import os,sys,base64
import folder_paths
import numpy as np

import torch,random
from comfy.model_management import get_torch_device
from huggingface_hub import snapshot_download

import torchaudio



# è·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
current_file_path = os.path.abspath(__file__)

# è·å–å½“å‰æ–‡ä»¶çš„ç›®å½•
current_directory = os.path.dirname(current_file_path)

# æ·»åŠ å½“å‰æ’ä»¶çš„nodesè·¯å¾„ï¼Œä½¿ChatTTSå¯ä»¥è¢«å¯¼å…¥ä½¿ç”¨
sys.path.append(current_directory)

       
from scipy.io import wavfile
import torch
from transformers import AutoProcessor, MusicgenForConditionalGeneration

from .utils import get_new_counter


modelpath=os.path.join(folder_paths.models_dir, "musicgen")


def init_audio_model(checkpoint):

    audio_processor = AutoProcessor.from_pretrained(checkpoint)

    audio_model = MusicgenForConditionalGeneration.from_pretrained(checkpoint)

    # audio_model.to(device)
    audio_model = audio_model.to(torch.device('cpu'))

    # increase the guidance scale to 4.0
    audio_model.generation_config.guidance_scale = 4.0

    # set the max new tokens to 256
    # 1500 - 30s
    audio_model.generation_config.max_new_tokens = 1500

    # set the softmax sampling temperature to 1.5
    audio_model.generation_config.temperature = 1.5

    return (audio_processor,audio_model)


class MusicNode:
    def __init__(self):
        self.audio_model = None
    @classmethod
    def INPUT_TYPES(s):
        return {"required": {
            "prompt": ("STRING", 
                         {
                            "multiline": True, 
                            "default": '',
                            "dynamicPrompts": True
                          }),
            
            "seconds":("FLOAT", {
                        "default": 5, 
                        "min": 1, #Minimum value
                        "max": 1000, #Maximum value
                        "step": 0.1, #Slider's step
                        "display": "number" # Cosmetic only: display as "number" or "slider"
                    }),
            "guidance_scale":("FLOAT", {
                        "default": 4.0, 
                        "min": 0, #Minimum value
                        "max": 20, #Maximum value
                    }),

            "seed":  ("INT", {"default": 0, "min": 0, "max": np.iinfo(np.int32).max}), 

            "device": (["auto","cpu"],),
                             },

            
                }
    
    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio",)

    FUNCTION = "run"

    CATEGORY = "â™¾ï¸Sound Lab"

    INPUT_IS_LIST = False
    OUTPUT_IS_LIST = (False,)
  
    def run(self,prompt,seconds,guidance_scale,seed,device):
        
        if seed==-1:
            seed = np.random.randint(0, np.iinfo(np.int32).max)

        # Set the seed for reproducibility
        torch.manual_seed(seed)
        random.seed(seed)
        np.random.seed(seed)

      
        if self.audio_model ==None:
            
            if os.path.exists(modelpath)==False:
                os.mkdir(modelpath)

            if os.path.exists(modelpath):

                config=os.path.join(modelpath,'config.json')
                if os.path.exists(config)==False:
                    snapshot_download("facebook/musicgen-small",
                                                local_dir=modelpath,
                                                # local_dir_use_symlinks=False,
                                                # filename="config.json",
                                                endpoint='https://hf-mirror.com')
                
                self.audio_processor,self.audio_model=init_audio_model(modelpath)
                
          
        inputs = self.audio_processor(
            text=prompt,
            # audio=audio,
            # sampling_rate=sampling_rate,
            padding=True,
            return_tensors="pt",
        )

        if device=='auto':
            device="cuda" if torch.cuda.is_available() else "cpu"

        self.audio_model.to(torch.device(device))

        # max_tokens=256 #default=5, le=30
        # if duration:
        #     max_tokens=int(duration*50)

        # seconds = 10  # ä¾‹å¦‚ï¼Œç”¨æˆ·è¾“å…¥çš„ç§’æ•°
        tokens_per_second = 1500 / 30
        max_tokens = int(tokens_per_second * seconds)


        sampling_rate = self.audio_model.config.audio_encoder.sampling_rate
        # input_audio
        audio_values = self.audio_model.generate(**inputs.to(device), 
                    do_sample=True, 
                    guidance_scale=guidance_scale, 
                    max_new_tokens=max_tokens,
                    )
        
        self.audio_model.to(torch.device('cpu'))

        audio=audio_values[0, 0].cpu().numpy()

        output_dir = folder_paths.get_output_directory()
    
        audio_file="music_gen"
        counter=get_new_counter(output_dir,audio_file)
        # print('#audio_path',folder_paths, )
        # æ·»åŠ æ–‡ä»¶ååç¼€
        audio_file = f"{audio_file}_{counter:05}.wav"
        
        audio_path=os.path.join(output_dir, audio_file)
 
        # save the best audio sample (index 0) as a .wav file
        wavfile.write(audio_path, rate=sampling_rate, data=audio)

        # with open(audio_path, "rb") as audio_file:
        #     audio_data = audio_file.read()
        #     audio_base64 = f'data:audio/wav;base64,'+base64.b64encode(audio_data).decode("utf-8")

        return ({
                "filename": audio_file,
                "subfolder": "",
                "type": "output",
                "prompt":prompt
                },)


class AudioPlayNode:
    def __init__(self):
        self.output_dir = folder_paths.get_temp_directory()
        self.type = "temp"
        self.prefix_append = ""
        self.compress_level = 4
    @classmethod
    def INPUT_TYPES(s):
        return {"required": {"audio": ("AUDIO",)}}

    RETURN_TYPES = ()
    FUNCTION = "run"
    CATEGORY = "â™¾ï¸Mixlab/Audio"
    INPUT_IS_LIST = False
    OUTPUT_IS_LIST = ()
    OUTPUT_NODE = True

    def run(self, audio):
        # åˆ¤æ–­æ˜¯å¦æ˜¯ Tensor ç±»å‹æˆ–åŒ…å«å¿…è¦çš„éŸ³é¢‘æ•°æ®
        is_tensor = isinstance(audio, dict) and 'waveform' in audio and 'sample_rate' in audio
        print('# åˆ¤æ–­æ˜¯å¦æ˜¯æœ‰æ•ˆçš„éŸ³é¢‘æ•°æ®', is_tensor, audio)

        results = []
        if is_tensor:
            filename_prefix = self.prefix_append
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(
                filename_prefix, self.output_dir)

            filename_with_batch_num = filename.replace("%batch_num%", str(1))
            file = f"{filename_with_batch_num}_{counter:05}_.wav"

            try:
                torchaudio.save(os.path.join(full_output_folder, file), audio['waveform'].squeeze(0),
                                audio["sample_rate"])
                results.append({
                    "filename": file,
                    "subfolder": subfolder,
                    "type": self.type
                })
            except Exception as e:
                print(f"Error saving audio file: {e}")
        else:
            # å¦‚æœä¸æ˜¯ Tensor ç±»å‹æˆ–æœ‰æ•ˆçš„éŸ³é¢‘å­—å…¸ï¼Œç›´æ¥è¿”å›è¾“å…¥
            results = [audio]

        # return {"ui": {"audio": results}}
        return {"ui": {"audio": results}}


class AudioToDictNode:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "audio": ("AUDIO",),  # æ¥å— AUDIO ç±»å‹çš„è¾“å…¥
            },
        }

    RETURN_TYPES = ("AUDIO",)  # å®šä¹‰è¿”å›ç±»å‹ä¸º AUDIO_DICT
    RETURN_NAMES = ("audioğŸ¥",)
    FUNCTION = "convert_to_dict"
    CATEGORY = "â™¾ï¸Mixlab/Audio"

    def convert_to_dict(self, audio):
        # æƒ…å†µ A: è¾“å…¥æ˜¯ tensor ç±»å‹
        if isinstance(audio, torch.Tensor):
            audio_dict = {
                "waveform": audio,
                "sample_rate": 32000  # é»˜è®¤é‡‡æ ·ç‡
            }
            return (audio_dict,)

        # æƒ…å†µ B: è¾“å…¥å·²ç»æ˜¯å®Œæ•´çš„å­—å…¸æ ¼å¼
        if isinstance(audio, dict) and 'waveform' in audio and 'sample_rate' in audio:
            return (audio,)  # ç›´æ¥è¿”å›åŸå§‹å­—å…¸æ ¼å¼

        # æƒ…å†µ C: è¾“å…¥æ˜¯åŒ…å«æ–‡ä»¶ä¿¡æ¯çš„å­—å…¸ï¼Œéœ€è¦è¯»å–éŸ³é¢‘æ–‡ä»¶
        if isinstance(audio, dict) and "filename" in audio:
            file_path = os.path.join(audio.get("subfolder", ""), audio["filename"])
            output_dir = folder_paths.get_output_directory()
            audio_path = os.path.join(output_dir, file_path)
            try:
                waveform, sample_rate = torchaudio.load(audio_path)
                # ç¡®ä¿ waveform æ˜¯ 3D å¼ é‡
                waveform = waveform.unsqueeze(0) if waveform.dim() == 2 else waveform
                audio_dict = {
                    "waveform": waveform,
                    "sample_rate": sample_rate
                }
                return (audio_dict,)
            except Exception as e:
                raise ValueError(f"Error loading audio file {file_path}: {e}")

        # å¦‚æœè¾“å…¥ä¸ç¬¦åˆä¸Šè¿°ä»»ä½•ä¸€ç§æƒ…å†µï¼ŒæŠ›å‡ºå¼‚å¸¸
        raise ValueError("Unsupported audio input format")